---
title: "Surdek Milestone Three"
author: "msurdek"
date: "9/22/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width = 100)
```

## 1) load required packages and data pre-processing functions

```{r, message=FALSE, warnings=FALSE, results=FALSE}
library(readr)
library(dplyr)
library(tidymodels)
library(caret)
library(glmnet)
library(corrplot)
library(car)
library(usmap)
source("/home/msurdek/Documents/School/SNHU/DAT 690 Capstone in Data Analytics/Assignments/Programming Revision/Churn Data Pre-Processing Functions.R")
```

## 2) read data files

```{r, message=FALSE, warnings=FALSE, results=FALSE}
# training data
data_churn <- read_churn_file("/home/msurdek/Documents/School/SNHU/DAT 690 Capstone in Data Analytics/Assignments/Project/Data/Churn Data.csv")
# data_churn <- read_csv("/home/msurdek/Documents/School/SNHU/DAT 690 Capstone in Data Analytics/Assignments/Project/Data/Churn Data.csv")
# data_churn <- data_churn |> 
#   select(!c(CREDITA,CREDITAA,CREDITB,CREDITC,CREDITDE,CREDITGY,CREDITZ,PRIZMRUR,PRIZMUB,PRIZMTWN,OCCPROF,OCCCLER,OCCCRFT,OCCSTUD,OCCHMKR,OCCRET,OCCSELF,OCC,MARRYUN,MARRYYES,MARRYNO,MARRY,INCMISS,SETPRCM,CALIBRAT,CHURNDEP))
# 
# data_churn <- data_churn |>
#   distinct(CUSTOMER, .keep_all=TRUE)
# summary(data_churn)
# testing data
data_churn_test <- read_churn_file("/home/msurdek/Documents/School/SNHU/DAT 690 Capstone in Data Analytics/Assignments/Project/Data/Churn Verification Data.csv")
```

## 3) handle missing data

```{r}
# convert zeros to in NAs in columns where that is how they are stored
data_churn <- data_churn |> 
  zeros_to_nas()

# calculate new values that will be imputed
revenue_median <- median(data_churn$REVENUE |> na.omit())
minutes_median <- median(data_churn$MOU |> na.omit())
recchrge_median <- median(data_churn$RECCHRGE |> na.omit())
directas_median <- median(data_churn$DIRECTAS |> na.omit())
overage_median <- median(data_churn$OVERAGE |> na.omit())
roaming_median <- median(data_churn$ROAM |> na.omit())
changem_zero <- 0
changer_zero <- 0
age1_median <- median(data_churn$AGE1 |> na.omit())
age2_median <- median(data_churn$AGE2 |> na.omit())
income_median <- median(data_churn$INCOME |> na.omit())
setprice_median <- median(data_churn$SETPRC |> na.omit())
  
# impute missing values into training and testing data
data_churn <- data_churn |> 
  impute_values()

data_churn_test <- data_churn_test |> 
  zeros_to_nas() |> 
  impute_values()
```

## 4) add derived features

```{r}
data_churn <- data_churn |> 
  get_attributes() |> 
  get_csa_info()

data_churn_test <- data_churn_test |> 
  get_attributes() |> 
  get_csa_info()
```

## 5) principal component analysis of some usage/subscription fields

```{r}
# select data for pca
usage_pca_vars <- c('DIRECTAS','OVERAGE','ROAM','CALLFWDV','REVENUE','RECCHRGE')

usage_pca_data <- data_churn |>
  select(c('CUSTOMER','CHURN_TARGET',all_of(usage_pca_vars)))

# create pca recipe
usage_pca_rec <- recipe(~., data = usage_pca_data) %>%
  update_role(CUSTOMER, CHURN_TARGET, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

# train pca
usage_pca_prep <- prep(usage_pca_rec)

# generate pca values for training and testing data
usage_pca_values <- juice(usage_pca_prep) |> 
  rename(USAGEPC2 = "PC2") |> 
  select('CUSTOMER','USAGEPC2')

usage_pca_values_test <- bake(usage_pca_prep, data_churn_test) |> 
  rename(USAGEPC2 = "PC2") |> 
  select('CUSTOMER','USAGEPC2')

data_churn <- data_churn |> 
  left_join(usage_pca_values, by = 'CUSTOMER')

data_churn_test <- data_churn_test |> 
  left_join(usage_pca_values_test, by = 'CUSTOMER')
```

## 6) model variables

```{r}
# select data for model
model_vars <- c('CUSTOMER','CHURN_TARGET','CHANGEM','RETCALL','PRIZMTWN','TRAVEL','CHILDREN','MOU_low','USAGEPC2','CSA_region_SEW','CSA_region_NS','CREDITDEGY')

working_model_data <- data_churn |>
  select(all_of(model_vars)) |> 
  select(!CUSTOMER)
```

## 7) correlation & model for collinearity

```{r}
# analyze correlation between predictors and VIF values of model inputs
pairs_data <- working_model_data |> select(!c('CHURN_TARGET'))

res <- pairs_data |> cor()

corrplot(res, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)

model <- glm(CHURN_TARGET ~.,family=binomial(link='logit'),data=working_model_data)

print(vif(model))
```

## 8) Regularized Logistic Regression Model

```{r}
set.seed(10)

train <- data_churn |>
  select(all_of(model_vars))

# upsample data to 50/50 churn&no-churn split
train <- upSample(x = train, y = train$CHURN_TARGET) |>
  select(!Class)

# remove some duplicate churn observations - createsa 45/55 percent split
train$X <- sample(rep(c(0, 1, 2, 3), nrow(train) / 4))

train <- train |>
  distinct(CUSTOMER, X, .keep_all=TRUE) |>
  select(!c(X,CUSTOMER))
#train |> tabyl(CHURN_TARGET)
# create model training matrices
data_train_x <- model.matrix(CHURN_TARGET~., train)[,-1]
data_train_y <- as.integer(train$CHURN_TARGET)

# determine optimal ratio of ridge/lasso regression through cross validation
cv.lasso <- cv.glmnet(data_train_x, data_train_y, alpha = 1, family = "binomial")

# train glm model
model1 <- glmnet(data_train_x, data_train_y, alpha = 1, family = "binomial",
                lambda = cv.lasso$lambda.min)

# print input coefficients
coef(model1)
```

## 9) model evaluation

```{r}
# select fields for model evaluation from testing data
data_test <- data_churn_test |>
  select(all_of(model_vars)) |> 
  select(!CUSTOMER)

x.test <- model.matrix(CHURN_TARGET ~., data_test)[,-1]

# apply model to testing data
probabilities <- model1 |> 
  predict(newx = x.test,type="response")

predicted.classes <- ifelse(probabilities > 0.5, 1, 0)

pred_results <- data_test |> 
  select(CHURN_TARGET) |> 
  bind_cols(predicted.classes) |> 
  mutate(predicted = as.factor(s0)) |> 
  select(CHURN_TARGET,predicted) |> 
  bind_cols(probabilities) |>
  rename(prob1 = 's0') |> 
  mutate(prob0 = 1 - prob1)

# confusion matrix of predicted v actual
conf_mat(pred_results, truth = CHURN_TARGET, estimate = predicted)

# other evaluation metrics - accuracy, sensitivity, etc.
custom_metrics <- metric_set(accuracy, sens, yardstick::spec, yardstick::precision, yardstick::recall, f_meas, kap, mcc)

custom_metrics(pred_results, truth = CHURN_TARGET, estimate = predicted)
```

## 10) Presentation Viz

```{r}
data_churn |> 
  ggplot(aes(x=INCOME, color = CHURN_TARGET)) +
  geom_density() +
  theme_bw() +
  scale_color_manual(values = c("royalblue","red"))

data_churn |> 
  group_by(PRIZM_LABEL) |> 
  summarize(count = n(), churn = sum(CHURN_INT), pct = churn/count)

data_churn |> 
  ggplot(aes(x = PRIZM_LABEL, fill = CHURN_TARGET)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("royalblue","red")) +
  theme_bw()

data_churn |> 
  ggplot(aes(x=CHANGEM, color = CHURN_TARGET)) +
  geom_density() +
  theme_bw() +
  scale_color_manual(values = c("royalblue","red"))

data_churn |> 
  ggplot(aes(x=MONTHS, color = CHURN_TARGET)) +
  geom_density() +
  theme_bw() +
  scale_color_manual(values = c("royalblue","red"))

data_churn |> 
  ggplot(aes(x=EQPDAYS, color = CHURN_TARGET)) +
  geom_density() +
  theme_bw() +
  scale_color_manual(values = c("royalblue","red"))

data_churn |> 
  mutate(REFURB = as.factor(REFURB)) |> 
  ggplot(aes(x = REFURB, fill = CHURN_TARGET)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("royalblue","red")) +
  theme_bw()

data_churn |> 
  mutate(RETCALL = as.factor(RETCALL)) |> 
  ggplot(aes(x = RETCALL, fill = CHURN_TARGET)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("royalblue","red")) +
  theme_bw()

data_churn |> 
  ggplot(aes(x=MOU, color = CHURN_TARGET)) +
  geom_density() +
  theme_bw() +
  scale_color_manual(values = c("royalblue","red"))

data_churn |> 
  ggplot(aes(x=MOU, fill = CHURN_TARGET)) +
  geom_histogram() +
  theme_bw() +
  scale_fill_manual(values = c("royalblue","red")) +
  facet_wrap(~CHURN_TARGET, nrow = 2, scales = "free_y")

#data_churn |> tabyl(CSA)

missing_df <- data.frame(state = c("AK","AR","DC","DE","ID","IA","KS","ME","MT","NH","NJ","ND","OR","RI","SC","SD","VT","WV","WY"),
                         region = c("Northwest","South","East","East","Northwest","North","Midwest","Northeast","Northwest","Northeast","East","North","Northwest","Northeast","Southeast","North","Northeast","East","Northwest"))

churn_by_region <- data_churn |> 
  group_by(CSA_region) |> 
  summarize(count = n(), churn = sum(CHURN_INT)) |> 
  mutate(pct = churn / count) |> 
  mutate(region = CSA_region) |> 
  select(c(region, pct))

states_by_region <- data_churn |> 
  group_by(CSA_state) |> 
  summarise(region = first(CSA_region)) |> 
  mutate(state = CSA_state) |> 
  select(!CSA_state) |> 
  rbind(missing_df) |> 
  left_join(churn_by_region, by = "region")

plot_usmap(data = states_by_region, values = "region") + 
  theme(legend.position = "right")

plot_usmap(data = states_by_region, values = "pct", color = "blue") + 
  scale_fill_continuous(low = "cadetblue1", high = "navy", name = "Churn PCT", label = scales::comma) + 
  theme(legend.position = "right")

summary(data_churn)
```

